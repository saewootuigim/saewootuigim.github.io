{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cyxg10HiCpj"
   },
   "source": [
    "Principal Component Analysis (PCA)\n",
    "==============================\n",
    "\n",
    "Let's say that we have a data set to train a machine learning algorithm. A data set usually takes the form of a two-dimensional array where every row is an instance, experiment, observation, or any term that may comfort you, and each column represents the variable, feature, parameter, or, as before, any name that may comfort you.\n",
    "\n",
    "Typically, not all columns are mutually independent; instead, they are correlated to each other to some degree. It means that not all columns may be needed when training an algorithm when there are columns that are highly correlated to one another. If there is a way to remove such correlation and make every column mutually independent, then we may be able to reduce the number of columns, i.e., reduce the dimension. It can be done by PCA.\n",
    "\n",
    "In mathematical language, PCA is an orthogonal projection of an $m$-dimensional vector on to some $l$-dimensional subspace, where $l\\leqslant m$. If $l$ is smaller than $m$, then we can benefit from the dimension reduction, while $l=m$ is still good because every column becomes linearly independent to each other after PCA so that the multicollinearity is naturally resolved.\n",
    "\n",
    "PCA typically follows the following steps\n",
    "\n",
    "### 1. Normalize\n",
    "\n",
    "Let's say that a data set $\\mathbf{X}^b\\in\\mathbb{R}^{n\\times m}$is given. The superscript $b$ stands for raw data. I will use superscript $a$ for normalized data and no superscript for the final data.\n",
    "\n",
    "$\\mathbf{X}^b$ has $n$ experimental data and $m$ variables. We want to train a machine learning algorithm with it. What we have to do is normalization of data so that the data to have the mean at 0 and a standard deviation of 1, i.e.:\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathbf{x}^a_j=\\frac{\\mathbf{x}^b_j-\\mathrm{mean}\\left(\\mathbf{x}^b_j\\right)\\mathbf{1}}{\\sigma\\left(\\mathbf{x}^b_j\\right)}\n",
    "\\end{gather}\n",
    "\n",
    "Here, $\\mathbf{x}^a_j\\in\\mathbb{R}^n$ is the normalized column vector, mean$\\left(\\mathbf{x}^b_j\\right)$ is the mean of $\\mathbf{x}^b_j$ which is a scalar, $\\mathbf{1}\\in\\mathbb{R}^n$ is a vector where all entries are 1, and $\\sigma\\left(\\mathbf{x}^b_j\\right)$ is the standard deviation of $\\mathbf{x}^b_j$. The resulting new matrix $\\mathbf{X}^a=\\left[~\\mathbf{x}^a_1~\\cdots~\\mathbf{x}^a_m~\\right]$ possesses identical information to $\\mathbf{X}^b$ but it is column-wise normalized.\n",
    "\n",
    "These are implemented as follows. First, read the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SEs--X70j2-g",
    "outputId": "89f3ed5e-c72c-40e4-f0e8-d3928d36a94d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
       "\n",
       "   Model Year  Origin  \n",
       "0          70       1  \n",
       "1          70       1  \n",
       "2          70       1  \n",
       "3          70       1  \n",
       "4          70       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This data set is from the University of California, Irvine Machine Learning Repository.\n",
    "# Last column of this data set is the make and name of the cars, which will not be used in this example.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\",names=column_names,na_values=\"?\",comment=\"\\t\",sep=\" \",skipinitialspace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "colab_type": "code",
    "id": "-0A1OACPLe8S",
    "outputId": "972bcead-18db-466c-f04b-e31ed2d6b1f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      6\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do some pre-processing on the data set before starting PCA.\n",
    "# There are nan values in the data.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fgANC6EmQ-rH",
    "outputId": "86257762-30af-40d2-acdd-25eafe743589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size was (398, 8) is now (392, 8)\n"
     ]
    }
   ],
   "source": [
    "# Let's remove them.\n",
    "import numpy as np\n",
    "\n",
    "a = df.shape\n",
    "df.dropna(inplace=True)\n",
    "print(\"Size was {}\".format(a)+\" is now {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "erq2Db7kaHcs",
    "outputId": "de07b074-ec15-4ab1-d44c-3ada7d3d36dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  Model Year\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0          70\n",
       "1  15.0          8         350.0       165.0  3693.0          11.5          70\n",
       "2  18.0          8         318.0       150.0  3436.0          11.0          70\n",
       "3  16.0          8         304.0       150.0  3433.0          12.0          70\n",
       "4  17.0          8         302.0       140.0  3449.0          10.5          70"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Origin column\" is categorical. Let's remove it from the data frame and attach it after PCA.\n",
    "origin = df.pop(\"Origin\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dds3pLvKiCpt"
   },
   "source": [
    "<p>We normalize this.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "uDXd_bE_iCpw",
    "outputId": "5b27f609-fb7b-4016-e625-bda32874cc56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.075915</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.486832</td>\n",
       "      <td>1.572585</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>-1.464852</td>\n",
       "      <td>-1.623241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.181033</td>\n",
       "      <td>1.182885</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>-1.646086</td>\n",
       "      <td>-1.623241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.953992</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.047246</td>\n",
       "      <td>1.182885</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825870</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>0.923085</td>\n",
       "      <td>0.554997</td>\n",
       "      <td>-1.827320</td>\n",
       "      <td>-1.623241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MPG  Cylinders  Displacement  Horsepower    Weight  Acceleration  \\\n",
       "0 -0.697747   1.482053      1.075915    0.663285  0.619748     -1.283618   \n",
       "1 -1.082115   1.482053      1.486832    1.572585  0.842258     -1.464852   \n",
       "2 -0.697747   1.482053      1.181033    1.182885  0.539692     -1.646086   \n",
       "3 -0.953992   1.482053      1.047246    1.182885  0.536160     -1.283618   \n",
       "4 -0.825870   1.482053      1.028134    0.923085  0.554997     -1.827320   \n",
       "\n",
       "   Model Year  \n",
       "0   -1.623241  \n",
       "1   -1.623241  \n",
       "2   -1.623241  \n",
       "3   -1.623241  \n",
       "4   -1.623241  "
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = (df-df.mean())/df.std()\n",
    "X_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhP7JDkFiCp1"
   },
   "source": [
    "### 2. Covariance Matrix\n",
    "\n",
    "Next, we need to compute the covariance between each column. The definition of covariance is\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathbf{Cov}=\\frac{1}{n-1}\\mathbf{X}^{aT}\\mathbf{X}^a\n",
    "\\end{gather}\n",
    "\n",
    "but the DataFrame class in `pandas` already has covariance method. So we do it in following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "GxH4yhcViCp2",
    "outputId": "84b4db75-66e4-4199-d134-d51118cf6690",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MPG</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.777618</td>\n",
       "      <td>-0.805127</td>\n",
       "      <td>-0.778427</td>\n",
       "      <td>-0.832244</td>\n",
       "      <td>0.423329</td>\n",
       "      <td>0.580541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cylinders</th>\n",
       "      <td>-0.777618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950823</td>\n",
       "      <td>0.842983</td>\n",
       "      <td>0.897527</td>\n",
       "      <td>-0.504683</td>\n",
       "      <td>-0.345647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Displacement</th>\n",
       "      <td>-0.805127</td>\n",
       "      <td>0.950823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897257</td>\n",
       "      <td>0.932994</td>\n",
       "      <td>-0.543800</td>\n",
       "      <td>-0.369855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horsepower</th>\n",
       "      <td>-0.778427</td>\n",
       "      <td>0.842983</td>\n",
       "      <td>0.897257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864538</td>\n",
       "      <td>-0.689196</td>\n",
       "      <td>-0.416361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>-0.832244</td>\n",
       "      <td>0.897527</td>\n",
       "      <td>0.932994</td>\n",
       "      <td>0.864538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.416839</td>\n",
       "      <td>-0.309120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acceleration</th>\n",
       "      <td>0.423329</td>\n",
       "      <td>-0.504683</td>\n",
       "      <td>-0.543800</td>\n",
       "      <td>-0.689196</td>\n",
       "      <td>-0.416839</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Year</th>\n",
       "      <td>0.580541</td>\n",
       "      <td>-0.345647</td>\n",
       "      <td>-0.369855</td>\n",
       "      <td>-0.416361</td>\n",
       "      <td>-0.309120</td>\n",
       "      <td>0.290316</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   MPG  Cylinders  Displacement  Horsepower    Weight  \\\n",
       "MPG           1.000000  -0.777618     -0.805127   -0.778427 -0.832244   \n",
       "Cylinders    -0.777618   1.000000      0.950823    0.842983  0.897527   \n",
       "Displacement -0.805127   0.950823      1.000000    0.897257  0.932994   \n",
       "Horsepower   -0.778427   0.842983      0.897257    1.000000  0.864538   \n",
       "Weight       -0.832244   0.897527      0.932994    0.864538  1.000000   \n",
       "Acceleration  0.423329  -0.504683     -0.543800   -0.689196 -0.416839   \n",
       "Model Year    0.580541  -0.345647     -0.369855   -0.416361 -0.309120   \n",
       "\n",
       "              Acceleration  Model Year  \n",
       "MPG               0.423329    0.580541  \n",
       "Cylinders        -0.504683   -0.345647  \n",
       "Displacement     -0.543800   -0.369855  \n",
       "Horsepower       -0.689196   -0.416361  \n",
       "Weight           -0.416839   -0.309120  \n",
       "Acceleration      1.000000    0.290316  \n",
       "Model Year        0.290316    1.000000  "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov = X_norm.cov()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "  display(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_wQjUvtiCp6"
   },
   "source": [
    "We see that the off-diagonal elements are very large (far from 0 and close to 1) which indicates the PCA would be meaningful.\n",
    "\n",
    "### 3. Eigenvalue Decomposition\n",
    "\n",
    "Now, it is time to analyze and obtain the principal components. Actually, the eigenvalue decomposition is everything you need to do.\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathbf{Cov}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^T.\n",
    "\\end{gather}\n",
    "\n",
    "Here, $\\mathbf{V}$ is a matrix that has eigenvectors of $\\mathbf{C}$ as its column vectors and $\\boldsymbol{\\Lambda}$ is a diagonal matrix having eigenvalues as its entries. Algebraic and geometric multiplicities are not strictly considered. Actually, for the real world data, it is rare to see such situation unless there are duplicated columns exist in the data set $\\mathbf{X}^b$.\n",
    "\n",
    "The eigenvalues are, in fact, the variance, or importance, of corresponding the eigenvector, and the corresponding eigenvector is the *recipe* to generate the corresponding prinicpal component: dot product of row $i$ in $\\mathbf{X}^a$ and eigenvector 1 yield principal component 1. In general\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathbf{x}_i=\\mathbf{x}^a_i\\mathbf{V}.\n",
    "\\end{gather}\n",
    "  \n",
    "To represent whole data by PCA, one should choose all the principal components to create a new data set. Such approach would not be able to reduce the dimension, however, this preprocessed data set is de-correlated and the learning algorithm should return better prediction with shorter training time. On the other hand, one could choose several significant PCs to approximately represent the data and reduce the dimension of input data set without loosing much information of $\\mathbf{X}$.\n",
    "\n",
    "`numpy` has `eig` method which does the work for you, but it is not sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "DXbqrvPOiCp7",
    "outputId": "83e2930e-6ea5-45d6-8ddc-6966cb9e596c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of col 0 = 1.0000, lambda=5.0106\n",
      "norm of col 1 = 1.0000, lambda=0.8656\n",
      "norm of col 2 = 1.0000, lambda=0.7284\n",
      "norm of col 3 = 1.0000, lambda=0.1839\n",
      "norm of col 4 = 1.0000, lambda=0.1219\n",
      "norm of col 5 = 1.0000, lambda=0.0543\n",
      "norm of col 6 = 1.0000, lambda=0.0353\n"
     ]
    }
   ],
   "source": [
    "w, v = np.linalg.eig(cov)\n",
    "for i in range(len(w)):\n",
    "    print(\"norm of col {:1d} = {:.4f}, lambda={:.4f}\".format(i,np.linalg.norm(v[:,i]),w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IzdPwf5iCp-"
   },
   "source": [
    "We want to choose the first a few significant principal components, which is an easy task if the decomposition is sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "Lu1FajTxiCp_",
    "outputId": "7c18b561-143b-4651-f3a6-ce4c1299276f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of col 0 = 1.0000, lambda=5.0106\n",
      "norm of col 1 = 1.0000, lambda=0.8656\n",
      "norm of col 2 = 1.0000, lambda=0.7284\n",
      "norm of col 3 = 1.0000, lambda=0.1839\n",
      "norm of col 4 = 1.0000, lambda=0.1219\n",
      "norm of col 5 = 1.0000, lambda=0.0543\n",
      "norm of col 6 = 1.0000, lambda=0.0353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eig0</th>\n",
       "      <th>eig1</th>\n",
       "      <th>eig2</th>\n",
       "      <th>eig3</th>\n",
       "      <th>eig4</th>\n",
       "      <th>eig5</th>\n",
       "      <th>eig6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.398135</td>\n",
       "      <td>0.206759</td>\n",
       "      <td>-0.257215</td>\n",
       "      <td>-0.750966</td>\n",
       "      <td>-0.340776</td>\n",
       "      <td>0.209759</td>\n",
       "      <td>0.092212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416124</td>\n",
       "      <td>0.198541</td>\n",
       "      <td>0.139159</td>\n",
       "      <td>-0.477306</td>\n",
       "      <td>0.493222</td>\n",
       "      <td>-0.332548</td>\n",
       "      <td>0.431716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429283</td>\n",
       "      <td>0.180362</td>\n",
       "      <td>0.100316</td>\n",
       "      <td>-0.297847</td>\n",
       "      <td>0.056581</td>\n",
       "      <td>0.142967</td>\n",
       "      <td>-0.812877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.422813</td>\n",
       "      <td>0.085242</td>\n",
       "      <td>-0.169684</td>\n",
       "      <td>0.042076</td>\n",
       "      <td>-0.711289</td>\n",
       "      <td>-0.522803</td>\n",
       "      <td>0.064385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.414046</td>\n",
       "      <td>0.224675</td>\n",
       "      <td>0.276103</td>\n",
       "      <td>0.107735</td>\n",
       "      <td>-0.265158</td>\n",
       "      <td>0.696518</td>\n",
       "      <td>0.367154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-0.006972</td>\n",
       "      <td>0.893308</td>\n",
       "      <td>-0.121124</td>\n",
       "      <td>-0.230755</td>\n",
       "      <td>-0.223785</td>\n",
       "      <td>-0.052799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.229510</td>\n",
       "      <td>0.909675</td>\n",
       "      <td>-0.037246</td>\n",
       "      <td>0.302435</td>\n",
       "      <td>0.088961</td>\n",
       "      <td>-0.128195</td>\n",
       "      <td>-0.051132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eig0      eig1      eig2      eig3      eig4      eig5      eig6\n",
       "0 -0.398135  0.206759 -0.257215 -0.750966 -0.340776  0.209759  0.092212\n",
       "1  0.416124  0.198541  0.139159 -0.477306  0.493222 -0.332548  0.431716\n",
       "2  0.429283  0.180362  0.100316 -0.297847  0.056581  0.142967 -0.812877\n",
       "3  0.422813  0.085242 -0.169684  0.042076 -0.711289 -0.522803  0.064385\n",
       "4  0.414046  0.224675  0.276103  0.107735 -0.265158  0.696518  0.367154\n",
       "5 -0.284897 -0.006972  0.893308 -0.121124 -0.230755 -0.223785 -0.052799\n",
       "6 -0.229510  0.909675 -0.037246  0.302435  0.088961 -0.128195 -0.051132"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.argsort(w)[::-1]\n",
    "w = w[idx]\n",
    "v = v[:,idx]\n",
    "for i in range(len(w)):\n",
    "    print(\"norm of col {:1d} = {:.4f}, lambda={:.4f}\".format(i,np.linalg.norm(v[:,i]),w[i]))\n",
    "column_names = [\"eig{:1d}\".format(i) for i in range(len(w))]\n",
    "V = pd.DataFrame(v,columns=column_names)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "  display(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF9-zf3ziCqB"
   },
   "source": [
    "Now, let's see how each principal components are important than the others. First, we convert the eigenvalues into the percentage, and also compute the cumulative percentages as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "LfyIYuBiiCqB",
    "outputId": "33bd6bd6-7ebd-4ee9-f4c3-8db9fbfd7374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  71.5805  71.5805\n",
      "1  12.3656  83.9461\n",
      "2  10.4056  94.3517\n",
      "3   2.6274  96.9791\n",
      "4   1.7417  98.7207\n",
      "5   0.7751  99.4959\n",
      "6   0.5041 100.0000\n"
     ]
    }
   ],
   "source": [
    "z = w/sum(w)*100\n",
    "z_cumul = np.cumsum(z)\n",
    "for i in range(len(z)):\n",
    "    print(\"{:d} {:8.4f} {:8.4f}\".format(i,z[i],z_cumul[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceh272ENiCqE"
   },
   "source": [
    "The first (0-th) principal component takes 67.20% of information while second has 11.80% and so on and so forth. If we include first 4 principal components, then we can represent more than 90% of information that was in the raw data $\\mathbf{X}^b$. Let's do some plotting to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "MrJEN1dUiCqF",
    "outputId": "9d855eac-3119-4298-82b6-910e5b06c541"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 107)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAADCCAYAAACPIuPqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "ax2 = plt.twinx()\n",
    "_ = plt.xticks(range(len(z)),[\"PC{:1d}\".format(i) for i in range(len(z))])\n",
    "\n",
    "\n",
    "plt.sca(ax1)\n",
    "plt.bar(range(len(z)),z)\n",
    "ax1.set_facecolor(\"None\")\n",
    "plt.grid(False)\n",
    "for i in range(len(z)):\n",
    "    plt.text(i,z[i]+1,\"{:.2f}\".format(z[i]),horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Percentage [%]\")\n",
    "plt.ylim([0,79])\n",
    "\n",
    "plt.sca(ax2)\n",
    "plt.plot(range(len(z)),z_cumul,color=\"C1\",marker=\"s\")\n",
    "ax1.set_facecolor(\"None\")\n",
    "plt.grid(False)\n",
    "for i in range(1,len(z)):\n",
    "    plt.text(i,z_cumul[i]-5,\"{:.2f}\".format(z_cumul[i]),horizontalalignment=\"center\",verticalalignment=\"top\")\n",
    "plt.ylabel(\"Cumulative Percentage [%]\")\n",
    "plt.ylim([0,107])\n",
    "\n",
    "# Uncomment below if you want to save the figure.\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"significance-r200.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZr733lYiCqN"
   },
   "source": [
    "Good. Since we only have 8 columns in $\\mathbf{X}^b$, we do not actually have to reduce the dimension, but when there are so many, PCA will tell you the number of principal components and its ability of data representation.\n",
    "\n",
    "### 4. Preprocessing\n",
    "\n",
    "The last step is to get PCA-ed data, $\\mathbf{X}$. Simply do\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathbf{X}=\\mathbf{X}^a\\mathbf{V}\n",
    "\\end{gather}\n",
    "\n",
    "and that's it. Columns of $\\mathbf{X}$ are the principal components and their importance are from the greatest (the first column) to the smallest (the last column). If $\\mathbf{V}$, which is sorted in descending order, is used as a whole, than $\\mathbf{X}$ has the same size as $\\mathbf{X}^b$, the raw file, but if some of the columns in $\\mathbf{V}$ are discarded, $\\mathbf{X}$ will have the same number of columns as the column-discarded $\\mathbf{V}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "DpwOWur2iCqP",
    "outputId": "287b6c63-6d3f-440b-e614-4a7198f615f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.631685</td>\n",
       "      <td>-0.927853</td>\n",
       "      <td>-0.533996</td>\n",
       "      <td>-0.744639</td>\n",
       "      <td>0.545311</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.121922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.489341</td>\n",
       "      <td>-0.804445</td>\n",
       "      <td>-0.648666</td>\n",
       "      <td>-0.494197</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>-0.206870</td>\n",
       "      <td>-0.097737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.966623</td>\n",
       "      <td>-0.880062</td>\n",
       "      <td>-0.957519</td>\n",
       "      <td>-0.718806</td>\n",
       "      <td>0.286542</td>\n",
       "      <td>-0.136413</td>\n",
       "      <td>0.059673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.906483</td>\n",
       "      <td>-0.960493</td>\n",
       "      <td>-0.582209</td>\n",
       "      <td>-0.530810</td>\n",
       "      <td>0.283589</td>\n",
       "      <td>-0.292865</td>\n",
       "      <td>0.124362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.900120</td>\n",
       "      <td>-0.951573</td>\n",
       "      <td>-1.053490</td>\n",
       "      <td>-0.564380</td>\n",
       "      <td>0.544107</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.170608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  2.631685 -0.927853 -0.533996 -0.744639  0.545311  0.094853  0.121922\n",
       "1  3.489341 -0.804445 -0.648666 -0.494197  0.035590 -0.206870 -0.097737\n",
       "2  2.966623 -0.880062 -0.957519 -0.718806  0.286542 -0.136413  0.059673\n",
       "3  2.906483 -0.960493 -0.582209 -0.530810  0.283589 -0.292865  0.124362\n",
       "4  2.900120 -0.951573 -1.053490 -0.564380  0.544107  0.001894  0.170608"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca = X_norm.dot(v)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IE90TVF8iCqU"
   },
   "source": [
    "That's it. We use $\\mathbf{X}$ instead of $\\mathbf{X}^a$ or $\\mathbf{X}^b$ for the machine learning.\n",
    "\n",
    "Let's check if the PCA-ed data doesn't have multicollinearity, i.e. covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "WNLibU3IiCqV",
    "outputId": "d90f23dc-b420-4c64-b832-f1e2bb5277a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.010636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054257</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5        6\n",
       "0  5.010636  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000\n",
       "1  0.000000  0.865591  0.000000  0.000000  0.000000  0.000000  0.00000\n",
       "2  0.000000  0.000000  0.728394  0.000000  0.000000  0.000000  0.00000\n",
       "3  0.000000  0.000000  0.000000  0.183915  0.000000  0.000000  0.00000\n",
       "4  0.000000  0.000000  0.000000  0.000000  0.121916  0.000000  0.00000\n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.054257  0.00000\n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.03529"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = X_pca.cov()\n",
    "C.values[C.values < 1e-14] = 0 # Supress small numbers to zero.\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "  display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tM86xgqliCqY"
   },
   "source": [
    "Yes it is. And the diagonal entries are indeed the eigenvalues. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "hQMi2kKCiCqZ",
    "outputId": "431a5a36-f292-491d-a309-15da77018502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda[0] = 5.0106\n",
      "lambda[1] = 0.8656\n",
      "lambda[2] = 0.7284\n",
      "lambda[3] = 0.1839\n",
      "lambda[4] = 0.1219\n",
      "lambda[5] = 0.0543\n",
      "lambda[6] = 0.0353\n"
     ]
    }
   ],
   "source": [
    "for i, eigval in enumerate(w):\n",
    "  print(\"lambda[{:1d}] = {:.4f}\".format(i,eigval))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PCA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
